cat > scripts/ingest_slacs.py <<'PY'
# -*- coding: utf-8 -*-
"""
Ingest SLACS (Bolton+ 2008; VizieR J/ApJ/682/964) tables into a clean, joined dataset.

Inputs (raw, immutable):
  data/external/slacs/slacs_table4_observed_systems_bolton2008.csv
    - observational data incl. redshifts zFG (lens) and zBG (source)
  data/external/slacs/slacs_table5_lens_models_bolton2008.csv
    - lens model params incl. bSIE (Einstein radius proxy in arcsec), Good/Ring flags

Outputs (derived, reproducible):
  data/processed/slacs/slacs_joined_clean.csv
  data/processed/slacs/slacs_joined_with_flags.csv
  data/processed/slacs/slacs_ingest_report.txt

Join strategy:
  - Primary join on normalized "Name"-like key: JHHMM±DDMM (e.g., J0037-0942).
  - If table4 uses an SDSS-style name ("SDSS JHHMMSS.ss±DDMMSS.s"), normalize to JHHMM±DDMM.
  - If table4 already uses JHHMM±DDMM, pass through.

Validation:
  - numeric finiteness for z_l, z_s, theta_E_arcsec
  - strict astrophysical ordering: z_s > z_l
  - z_l >= 0, z_s >= 0, theta_E_arcsec > 0
  - flags preserved; invalid rows can be kept in *_with_flags.csv, but excluded from *_clean.csv

Usage:
  python scripts/ingest_slacs.py

Notes:
  - This script does not compute the sensitivity metric; it creates the canonical joined input table.
  - Keep raw files unchanged; transform only here.
"""

from __future__ import annotations

import os
import re
from pathlib import Path

import pandas as pd


ROOT = Path(__file__).resolve().parents[1]
DATA_EXT = ROOT / "data" / "external" / "slacs"
DATA_OUT = ROOT / "data" / "processed" / "slacs"

TABLE4_DEFAULT = DATA_EXT / "slacs_table4_observed_systems_bolton2008.csv"
TABLE5_DEFAULT = DATA_EXT / "slacs_table5_lens_models_bolton2008.csv"

OUT_CLEAN = DATA_OUT / "slacs_joined_clean.csv"
OUT_FLAGS = DATA_OUT / "slacs_joined_with_flags.csv"
OUT_REPORT = DATA_OUT / "slacs_ingest_report.txt"


_SDSS_J_RE = re.compile(
    r"(?i)\b(?:SDSS\s*)?J(\d{2})(\d{2})(\d{2}(?:\.\d+)?)\s*([+\-−])\s*(\d{2})(\d{2})(\d{2}(?:\.\d+)?)\b"
)
_JSHORT_RE = re.compile(r"(?i)\bJ(\d{4})\s*([+\-−])\s*(\d{4})\b")


def normalize_slacs_name(raw: str) -> str | None:
    """
    Normalize a SLACS system name into JHHMM±DDMM.

    Examples:
      "SDSS J003753.21-094220.1" -> "J0037-0942"
      "J0037-0942"               -> "J0037-0942"
      "J0037−0942"               -> "J0037-0942"  (unicode minus)

    Returns None if it cannot parse.
    """
    if raw is None:
        return None
    s = str(raw).strip()
    if not s:
        return None

    # Replace unicode minus with ASCII hyphen for normalization.
    s2 = s.replace("−", "-").replace("–", "-").replace("—", "-")

    # Already short form?
    m = _JSHORT_RE.search(s2)
    if m:
        hhmm = m.group(1)
        sign = "-" if m.group(2) in ("-", "−") else "+"
        ddmm = m.group(3)
        return f"J{hhmm}{sign}{ddmm}"

    # SDSS long form
    m = _SDSS_J_RE.search(s2)
    if m:
        hh = m.group(1)
        mm = m.group(2)
        sign = "-" if m.group(4) in ("-", "−") else "+"
        dd = m.group(5)
        dm = m.group(6)
        # Drop seconds -> JHHMM±DDMM
        return f"J{hh}{mm}{sign}{dd}{dm}"

    # Fallback: strip whitespace and try again
    s3 = re.sub(r"\s+", "", s2)
    m = _JSHORT_RE.search(s3)
    if m:
        hhmm = m.group(1)
        sign = "-" if m.group(2) in ("-", "−") else "+"
        ddmm = m.group(3)
        return f"J{hhmm}{sign}{ddmm}"

    return None


def _is_nan(x: float) -> bool:
    return x != x


def _is_inf(x: float) -> bool:
    return x == float("inf") or x == float("-inf")


def _to_float(x, flags: list[str], name: str) -> float | None:
    if x is None or (isinstance(x, float) and _is_nan(x)):
        flags.append(f"flag_missing_{name}")
        return None
    try:
        v = float(x)
    except Exception:
        flags.append(f"flag_non_numeric_{name}")
        return None
    if _is_nan(v):
        flags.append(f"flag_nan_{name}")
        return None
    if _is_inf(v):
        flags.append(f"flag_inf_{name}")
        return None
    return v


def _coerce_boolish(x) -> str:
    """Preserve VizieR 'Yes/No/?' style values as strings."""
    if x is None:
        return ""
    return str(x).strip()


def resolve_table4_columns(df4: pd.DataFrame) -> dict:
    """
    Identify name and redshift columns in table4.
    Expected: Name/System, zFG (lens), zBG (source).
    """
    lower = {c.lower(): c for c in df4.columns}

    name_col = None
    for candidate in ("name", "system", "obj", "object"):
        if candidate in lower:
            name_col = lower[candidate]
            break
    if name_col is None:
        for c in df4.columns:
            lc = c.lower()
            if "name" in lc or "system" in lc:
                name_col = c
                break

    zfg_col = lower.get("zfg", None)
    zbg_col = lower.get("zbg", None)

    for c in df4.columns:
        lc = c.lower()
        if zfg_col is None and lc in ("zl", "z_l", "z_lens"):
            zfg_col = c
        if zbg_col is None and lc in ("zs", "z_s", "z_source"):
            zbg_col = c

    return {"name": name_col, "zfg": zfg_col, "zbg": zbg_col}


def resolve_table5_columns(df5: pd.DataFrame) -> dict:
    """
    Identify name + Einstein-radius proxy columns in table5.
    Expected: Name/System and bSIE (arcsec). Optional: Good, Ring?.
    """
    lower = {c.lower(): c for c in df5.columns}

    name_col = None
    for candidate in ("name", "system", "obj", "object"):
        if candidate in lower:
            name_col = lower[candidate]
            break
    if name_col is None:
        for c in df5.columns:
            if "name" in c.lower():
                name_col = c
                break

    bsie_col = lower.get("bsie", None)
    if bsie_col is None:
        for c in df5.columns:
            if c.replace(" ", "").lower() == "bsie":
                bsie_col = c
                break

    good_col = None
    ring_col = None
    for c in df5.columns:
        lc = c.lower()
        if good_col is None and "good" in lc:
            good_col = c
        if ring_col is None and "ring" in lc:
            ring_col = c

    return {"name": name_col, "bsie": bsie_col, "good": good_col, "ring": ring_col}


def validate_row(theta_E_arcsec, z_l, z_s) -> tuple[bool, list[str], dict]:
    flags: list[str] = []
    norm: dict = {}

    th = _to_float(theta_E_arcsec, flags, "thetaE_arcsec")
    zl = _to_float(z_l, flags, "z_l")
    zs = _to_float(z_s, flags, "z_s")

    if th is not None and th <= 0.0:
        flags.append("flag_thetaE_nonpositive")
    if zl is not None and zl < 0.0:
        flags.append("flag_zl_negative")
    if zs is not None and zs < 0.0:
        flags.append("flag_zs_negative")
    if (zl is not None) and (zs is not None) and not (zs > zl):
        flags.append("flag_zs_le_zl")

    is_valid = len(flags) == 0
    if is_valid:
        norm = {"theta_E_arcsec": float(th), "z_l": float(zl), "z_s": float(zs)}

    return is_valid, flags, norm


def main():
    DATA_OUT.mkdir(parents=True, exist_ok=True)

    table4_path = Path(os.environ.get("SLACS_TABLE4", str(TABLE4_DEFAULT)))
    table5_path = Path(os.environ.get("SLACS_TABLE5", str(TABLE5_DEFAULT)))

    if not table4_path.exists():
        raise SystemExit(f"Missing table4 file: {table4_path}")
    if not table5_path.exists():
        raise SystemExit(f"Missing table5 file: {table5_path}")

    df4 = pd.read_csv(table4_path)
    df5 = pd.read_csv(table5_path)

    c4 = resolve_table4_columns(df4)
    c5 = resolve_table5_columns(df5)

    missing = []
    if c4["name"] is None: missing.append("table4:name")
    if c4["zfg"] is None:  missing.append("table4:zFG")
    if c4["zbg"] is None:  missing.append("table4:zBG")
    if c5["name"] is None: missing.append("table5:name")
    if c5["bsie"] is None: missing.append("table5:bSIE")
    if missing:
        raise SystemExit(
            "Could not resolve required columns:\n  - " + "\n  - ".join(missing) +
            "\n\nTip: print(df.columns) for each file and adjust resolver logic if needed."
        )

    df4 = df4.copy()
    df5 = df5.copy()
    df4["slacs_id"] = df4[c4["name"]].apply(normalize_slacs_name)
    df5["slacs_id"] = df5[c5["name"]].apply(normalize_slacs_name)

    n4_bad = int(df4["slacs_id"].isna().sum())
    n5_bad = int(df5["slacs_id"].isna().sum())

    df4j = df4.dropna(subset=["slacs_id"])
    df5j = df5.dropna(subset=["slacs_id"])

    df4_min = pd.DataFrame({
        "slacs_id": df4j["slacs_id"],
        "name_table4": df4j[c4["name"]].astype(str),
        "z_l": df4j[c4["zfg"]],
        "z_s": df4j[c4["zbg"]],
    })

    df5_min = pd.DataFrame({
        "slacs_id": df5j["slacs_id"],
        "name_table5": df5j[c5["name"]].astype(str),
        "theta_E_arcsec": df5j[c5["bsie"]],
        "Good": df5j[c5["good"]] if c5["good"] is not None else "",
        "Ring": df5j[c5["ring"]] if c5["ring"] is not None else "",
    })

    joined = df5_min.merge(df4_min, on="slacs_id", how="inner")

    out_rows = []
    for _, row in joined.iterrows():
        is_valid, flags, norm = validate_row(
            theta_E_arcsec=row.get("theta_E_arcsec"),
            z_l=row.get("z_l"),
            z_s=row.get("z_s"),
        )
        out = dict(row)
        out["is_valid"] = bool(is_valid)
        out["flags"] = ";".join(flags)
        out["theta_E_arcsec_norm"] = norm.get("theta_E_arcsec", "")
        out["z_l_norm"] = norm.get("z_l", "")
        out["z_s_norm"] = norm.get("z_s", "")
        out_rows.append(out)

    out_df = pd.DataFrame(out_rows)
    out_df.to_csv(OUT_FLAGS, index=False)

    clean = out_df[out_df["is_valid"] == True].copy()
    clean = clean.rename(columns={
        "theta_E_arcsec_norm": "theta_E_arcsec",
        "z_l_norm": "z_l",
        "z_s_norm": "z_s",
    })
    keep = ["slacs_id", "theta_E_arcsec", "z_l", "z_s", "Good", "Ring", "name_table4", "name_table5"]
    clean = clean[keep]
    clean.to_csv(OUT_CLEAN, index=False)

    total4 = len(df4)
    total5 = len(df5)
    joined_n = len(out_df)
    valid_n = len(clean)
    invalid_n = joined_n - valid_n

    flag_counts: dict[str, int] = {}
    for fstr in out_df["flags"].fillna("").astype(str):
        if not fstr:
            continue
        for f in [x for x in fstr.split(";") if x]:
            flag_counts[f] = flag_counts.get(f, 0) + 1

    set4 = set(df4_min["slacs_id"].astype(str).values)
    set5 = set(df5_min["slacs_id"].astype(str).values)
    only4 = len(set4 - set5)
    only5 = len(set5 - set4)

    lines = []
    lines.append("SLACS ingest report")
    lines.append("==================")
    lines.append(f"table4 path: {table4_path}")
    lines.append(f"table5 path: {table5_path}")
    lines.append("")
    lines.append(f"table4 rows: {total4} (unparsed names: {n4_bad})")
    lines.append(f"table5 rows: {total5} (unparsed names: {n5_bad})")
    lines.append("")
    lines.append(f"join rows (inner): {joined_n}")
    lines.append(f"valid rows:       {valid_n}")
    lines.append(f"invalid rows:     {invalid_n}")
    lines.append("")
    lines.append(f"IDs only in table4 (after name parse): {only4}")
    lines.append(f"IDs only in table5 (after name parse): {only5}")
    lines.append("")
    lines.append("Flag counts:")
    if flag_counts:
        for k in sorted(flag_counts.keys()):
            lines.append(f"  {k}: {flag_counts[k]}")
    else:
        lines.append("  (none)")
    lines.append("")
    lines.append(f"Wrote: {OUT_FLAGS}")
    lines.append(f"Wrote: {OUT_CLEAN}")
    lines.append(f"Wrote: {OUT_REPORT}")

    OUT_REPORT.write_text("\n".join(lines), encoding="utf-8")
    print("\n".join(lines))


if __name__ == "__main__":
    main()
PY

